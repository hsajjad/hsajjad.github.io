<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pages | Hassan Sajjad</title>
    <link>//localhost:1313/pages/</link>
      <atom:link href="//localhost:1313/pages/index.xml" rel="self" type="application/rss+xml" />
    <description>Pages</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 28 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>//localhost:1313/img/icon-192.png</url>
      <title>Pages</title>
      <link>//localhost:1313/pages/</link>
    </image>
    
    <item>
      <title>News</title>
      <link>//localhost:1313/pages/news/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/news/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Released:&lt;/strong&gt; First-ever &lt;a href=&#34;http://mt.qcri.org/api&#34; target=&#34;_blank&#34;&gt;Dialectal Arabic to English Machine Translation System&lt;/a&gt; covering a large variety of Arabic dialects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Licensed:&lt;/strong&gt; &lt;a href=&#34;http://mt.qcri.org/api&#34; target=&#34;_blank&#34;&gt;Machine Translation&lt;/a&gt; Technology to &lt;a href=&#34;http://www.kanerai.com&#34; target=&#34;_blank&#34;&gt;KanariAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @ACL 2020:&lt;/strong&gt; Similarity Analysis of Contextual Word Representation Models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; Interpreting Deep NLP Models, University of Edinburgh, UK (April 2020)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; University of Sheffield, UK (Mar 2020)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Invited talk:&lt;/strong&gt; Efficient Transfer Learning of Pretrained Model. 7th International Conference on Language and Technology, Pakistan (February 2020)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; Analyzing Individual Neurons in Deep NLP Models at Google, Facebook, Amazon, Salesforce and Bosch, US (April 2019)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keynote:&lt;/strong&gt; Hidden Linguistic in Deep NLP Models, Symposium on Natural Language Processing, University of Moratuwa, Sri Lanka (March 2019)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep learning for NLP:&lt;/strong&gt; Fun teaching at International Spring School in Advanced Language Engineering, University of Moratuwa (March 2019)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; Machine Translation in Real World, King&amp;rsquo;s College London, UK (March 2019)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; Analyzing Individual Neurons in Deep NLP Models, University of Melbourne, Australia (February 2019)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @NAACL 2019:&lt;/strong&gt; &amp;ldquo;One Size Does Not Fit All: Comparing NMT Representations of Different Granularities&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @NAACL 2019:&lt;/strong&gt; &amp;ldquo;Highly Effective Arabic Diacritization using Sequence to Sequence Modeling&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media coverage:&lt;/strong&gt; Our work on NeuroX - Analyzing and Controlling Individual Neurons is featured at &lt;a href=&#34;https://news.mit.edu/2019/neural-networks-nlp-microscope-0201&#34; target=&#34;_blank&#34;&gt;MIT news&lt;/a&gt; and several AI blogs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @ICLR 2019:&lt;/strong&gt; &amp;ldquo;Identifying and Controlling Important Neurons in Neural Machine Translation&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @AAAI 2019:&lt;/strong&gt; &amp;ldquo;What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @AAAI 2019:&lt;/strong&gt; &amp;ldquo;NeuroX: A toolkit for Analyzing Individual Neurons in Neural Network&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best Innovation Award&lt;/strong&gt; @&lt;a href=&#34;https://www.qf-arc.org/en-us/ARC-18&#34; target=&#34;_blank&#34;&gt;ARC&amp;rsquo;18&lt;/a&gt; for our &lt;a href=&#34;http://st.qcri.org&#34; target=&#34;_blank&#34;&gt;Speech Translation System&lt;/a&gt;, see &lt;a href=&#34;http://www.gulf-times.com/story/585737/Sheikha-Moza-honours-QF-ARC-18-award-winners&#34; target=&#34;_blank&#34;&gt;media coverage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep learning for NLP:&lt;/strong&gt; I will be delivering an intensive course on deep learning for NLP at the University of Duisburg-Essen, Germany in the second week of April 2018&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @NAACL 2018:&lt;/strong&gt; Incremental decoding and training methods for simultaneous translation in neural machine translation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media coverage:&lt;/strong&gt; Our work on understanding Neural Machine Translation has made it to &lt;a href=&#34;http://news.mit.edu/2017/reading-neural-network-mind-1211&#34; target=&#34;_blank&#34;&gt;MIT news&lt;/a&gt; and been picked by several channels like &lt;a href=&#34;https://scienceblog.com/498095/reading-neural-networks-mind/&#34; target=&#34;_blank&#34;&gt;ScienceBlog&lt;/a&gt;, &lt;a href=&#34;https://www.sciencedaily.com/releases/2017/12/171211120131.htm&#34; target=&#34;_blank&#34;&gt;ScienceDaily&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @IWSLT 2017:&lt;/strong&gt; Neural Machine Translation Training in a Multi-domain Scenario&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep learning for MT:&lt;/strong&gt; I had great fun in teaching deep learning for machine translation at the &lt;a href=&#34;http://cl-fallschool2017.phil.hhu.de/&#34; target=&#34;_blank&#34;&gt;DGfS-CL Fall School&lt;/a&gt;. Course material is available &lt;a href=&#34;http://cl-fallschool2017.phil.hhu.de/index.php/courses/course-2/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted two papers @ACL 2017&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Read my &lt;a href=&#34;https://www.linkedin.com/pulse/what-do-neural-machine-translation-models-learn-hassan-sajjad/&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt; about What does Neural Machine Translation Learn about Morphology&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning for Machine Translation</title>
      <link>//localhost:1313/pages/dl4mt/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/dl4mt/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Lecturers: &lt;a href=&#34;https://hsajjad.github.io/&#34; target=&#34;_blank&#34;&gt;Hassan Sajjad&lt;/a&gt; and &lt;a href=&#34;https://fdalvi.github.io/&#34; target=&#34;_blank&#34;&gt;Fahim Dalvi&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;In this lecture series, we first cover the basics of statistical machine translation to establish the intuition behind machine translation. We then cover the basics of neural network models - word embedding and neural language model. Finally, we learn an end-to-end translation system based completely on deep neural networks. In the last part of the lecture series, we learn to peek into these neural systems and analyze what they learn about the intricacies of a language like morphology and syntax, without ever explicitly seeing these details in the training data.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Background reading&lt;/strong&gt;
* &lt;a href=&#34;https://cs231n.github.io/python-numpy-tutorial/&#34; target=&#34;_blank&#34;&gt;Python Numpy Tutorial&lt;/a&gt;
* &lt;a href=&#34;https://cs231n.github.io/ipython-tutorial/&#34; target=&#34;_blank&#34;&gt;IPython Tutorial&lt;/a&gt;
* &lt;a href=&#34;http://www.cedar.buffalo.edu/~srihari/CSE574/Chap1/LinearAlgebra.pdf&#34; target=&#34;_blank&#34;&gt;Linear Aljebra for Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lecture 0&lt;/strong&gt; - Introduction &amp;amp; Roadmap &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzYkYxYzJaZUY0anc&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 1&lt;/strong&gt; - Language &amp;amp; Translation &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzZkhrcG9mSWo0cEE&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 2&lt;/strong&gt; - Language Modeling &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzSjhoanNaQTdlNDQ&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzTnc5d1VWa3ItbUU&#34; target=&#34;_blank&#34;&gt;Python tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzMDRKbTNWWk96V3M&#34; target=&#34;_blank&#34;&gt;Python tutorial as a PDF&lt;/a&gt; [non-editable]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 3&lt;/strong&gt; - Machine Learning &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzWDhUbDV6T3hZTDg&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzU2RjOFFQUW5IQk0&#34; target=&#34;_blank&#34;&gt;Decision Boundary Exercise&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzYUxKMGY2ZkluQ3c&#34; target=&#34;_blank&#34;&gt;Optimization functions demonstration&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 4&lt;/strong&gt; - Machine Learning II &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzSllqVWFXemUyNk0&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzTDVjdWJ2NFNxaTQ&#34; target=&#34;_blank&#34;&gt;Linear Classifier with MSE&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzeVBDOWJ0ckhQQjg&#34; target=&#34;_blank&#34;&gt;Linear Classifier with Softmax and Cross Entropy&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 5&lt;/strong&gt; - Machine Learning and Neural Networks &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzSmJjR2wwZjJjbEU&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzQVZtVmZsNTNqNE0&#34; target=&#34;_blank&#34;&gt;Efficient Linear Classifier with Softmax and Cross Entropy&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzwwwL3SyiJYVlAta1JhSk5pLXc&#34; target=&#34;_blank&#34;&gt;Neural Network&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzLU00MjZJQ2tXa1k&#34; target=&#34;_blank&#34;&gt;Neural Network with Keras toolkit&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 6&lt;/strong&gt; - Neural Network Language Models &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzazUyZjdPVkZYRGc&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzMVE4d1o0YlVtbzA&#34; target=&#34;_blank&#34;&gt;Language Modeling with Keras&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 7&lt;/strong&gt; - Sequence to Sequence &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzdjIwSWI2Mnk1ZHc&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 8&lt;/strong&gt; - Practical Neural MT &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzNjFSOUYwZ0txUzA&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 9&lt;/strong&gt; - Analysis of Neural MT &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzME1GWVlnWUdCREU&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 10&lt;/strong&gt; - Recent Advancements &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzS0xPWFlOT0ZOeG8&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning for Natural Language Processing</title>
      <link>//localhost:1313/pages/dl4nlp/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/dl4nlp/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Lecturers: &lt;a href=&#34;https://fdalvi.github.io/&#34; target=&#34;_blank&#34;&gt;Fahim Dalvi&lt;/a&gt; and &lt;a href=&#34;https://hsajjad.github.io/&#34; target=&#34;_blank&#34;&gt;Hassan Sajjad&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;In this lecture series, we cover the basics of machine learning, neural networks and deep neural networks. We look at several deep neural network architectures from the perspective of applying them to various classification tasks, such as sequence prediction and generation. Every lecture is accompanied with practice problems implemented in Keras, a popular Python framework for deep learning.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Background reading&lt;/strong&gt;
* &lt;a href=&#34;https://cs231n.github.io/python-numpy-tutorial/&#34; target=&#34;_blank&#34;&gt;Python Numpy Tutorial&lt;/a&gt;
* &lt;a href=&#34;https://cs231n.github.io/ipython-tutorial/&#34; target=&#34;_blank&#34;&gt;IPython Tutorial&lt;/a&gt;
* &lt;a href=&#34;http://www.cedar.buffalo.edu/~srihari/CSE574/Chap1/LinearAlgebra.pdf&#34; target=&#34;_blank&#34;&gt;Linear Aljebra for Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lecture 0&lt;/strong&gt; - Introduction &amp;amp; Roadmap &lt;a href=&#34;https://drive.google.com/open?id=1SUaoBDjAoFG2f_SAX2FNqM4Wcs3JeXbs&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 1&lt;/strong&gt; - Introduction to Machine Learning &lt;a href=&#34;https://drive.google.com/open?id=1U0289zrCDrABLpTZOUba_IcDoApq1tZT&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Practical session &lt;a href=&#34;https://drive.google.com/open?id=1nZTTXiHOlXo5HO4wRy88I5Sk0-_egGwI&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning Rate and Optimization Demo &lt;a href=&#34;https://drive.google.com/open?id=1VDBHBh_hGRiiZfIL61b7-RRm6r05Hkg0&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Introduction to Jupyter Notebooks &lt;a href=&#34;https://drive.google.com/open?id=15re_PMcGbg-I-UULld_FJZIIT6OC53A6&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Introduction to Python and Numpy &lt;a href=&#34;https://drive.google.com/open?id=178OeAeD_n-T_MChwiEbkbvZfquko3EZh&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linear Classification by Regression &lt;a href=&#34;https://drive.google.com/open?id=1PbIKa_u4Y19heQOa0dIenZWODNi354W-&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Binary Linear Classification &lt;a href=&#34;https://drive.google.com/open?id=1GIBoQxs1zlAauI8fY5vX2LDDTtkUXx8C&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linear Classification on Spiral Data &lt;a href=&#34;https://drive.google.com/open?id=1ZkmhRrI6eISdJppCVTKJ5cbhVYDg8uU5&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Supplementary Material &lt;a href=&#34;https://drive.google.com/open?id=10RjjHIY9W38YwzwiJ_6CPI19qpP1_4L_&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 2&lt;/strong&gt; - Neural Networks &lt;a href=&#34;https://drive.google.com/open?id=19m3Cf6_AqjmAZZi-n1gMfwYqUdIaFw7G&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Practical session &lt;a href=&#34;https://drive.google.com/open?id=1sYQLeSDxl_i7s5mzoeY5b6kPuxLP36d1&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Networks on Spiral Data &lt;a href=&#34;https://drive.google.com/open?id=1vAGPhE1w7Vc3u0OBBSpdCdwzEqa6Vl0H&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data &lt;a href=&#34;https://drive.google.com/open?id=1V0ioyI79Qf-Bc4FTs6MFylTGWDMzwMbZ&#34; target=&#34;_blank&#34;&gt;.zip&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sentiment Classification using Neural Networks &lt;a href=&#34;https://drive.google.com/open?id=11CticibncR7YqF40CwC8I2qUqmMfuRwn&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Network Language Model &lt;a href=&#34;https://drive.google.com/open?id=1uWQ8NWtKqESY5WC7VRrz9fnRJFykmos3&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 3&lt;/strong&gt; - Recurrent Neural Networks &lt;a href=&#34;https://drive.google.com/open?id=1jeQCTTxZkJ0E1R9kjGv1Upx4y2g1iMz5&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Practical Session &lt;a href=&#34;https://drive.google.com/open?id=1_AURYVRlVsEpIrK1yzkho_rO1E4mQRfD&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recurrent Neural Networks &lt;a href=&#34;https://drive.google.com/open?id=1_AURYVRlVsEpIrK1yzkho_rO1E4mQRfD&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hybrid Model &lt;a href=&#34;https://drive.google.com/open?id=1fe5HKrE2KMrLUM68ObrjM1gbB_dSRZE8&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 4&lt;/strong&gt; - Sequence to Sequence Models and Practical Considerations

&lt;ul&gt;
&lt;li&gt;Sequence to Sequence Models &lt;a href=&#34;https://drive.google.com/open?id=19PtpBXrIoj7v2Ux-Tw8zmnd5CNTK59OH&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Practical Considerations &lt;a href=&#34;https://drive.google.com/open?id=1V3XD5_aLpbTOP90mgqsL-EtxZL2PYHME&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;
&lt;!---  * Practical Session - Per Timestep Prediction  [English POS tagged data](#)  (https://drive.google.com/open?id=1iaN-h5oepAfeWas6bMH8t4RzbBkOccR9)  ---&gt;&lt;/li&gt;
&lt;li&gt;Per Timestep Prediction &lt;a href=&#34;https://drive.google.com/open?id=13L75QInaEmpF12YptSvGqvKwLGgytj7k&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pretrained Embeddings &lt;a href=&#34;https://drive.google.com/open?id=1fiYX7cXKSZhcpNuuO_vkvOO_QDyIEEWT&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Imbalanced Classes &lt;a href=&#34;https://drive.google.com/open?id=1ND2Gqjo5mDuG0H9O58yLuMsNsKhWC2g1&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 5&lt;/strong&gt; - Advanced Topics, CNN, Multitask, GAN, RL, etc. &lt;a href=&#34;https://drive.google.com/open?id=1vimK8YQXJLP1z2dxZUHph-pW40hJ8FEg&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Machine Translation</title>
      <link>//localhost:1313/pages/mt/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/mt/</guid>
      <description>

&lt;hr /&gt;

&lt;h3 id=&#34;highlights&#34;&gt;Highlights&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://st.qcri.org/demos/livetranslation/&#34; target=&#34;_blank&#34;&gt;Live Speech Translation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;First industry scale &lt;a href=&#34;https://mt.qcri.org/api/&#34; target=&#34;_blank&#34;&gt;dialectal Arabic to English machine translation system&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Machine Translation licensed to &lt;a href=&#34;http://www.kanerai.com&#34; target=&#34;_blank&#34;&gt;KanariAI&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Speech Translation System won the best Innovation award at @&lt;a href=&#34;https://www.qf-arc.org/en-us/ARC-18&#34; target=&#34;_blank&#34;&gt;ARC&amp;rsquo;18&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;best-innovation-award-arc-18-https-www-qf-arc-org-en-us-arc-18-media-coverage-http-www-gulf-times-com-story-585737-sheikha-moza-honours-qf-arc-18-award-winners&#34;&gt;Best Innovation Award @&lt;a href=&#34;https://www.qf-arc.org/en-us/ARC-18&#34; target=&#34;_blank&#34;&gt;ARC&amp;rsquo;18&lt;/a&gt;, &lt;a href=&#34;http://www.gulf-times.com/story/585737/Sheikha-Moza-honours-QF-ARC-18-award-winners&#34; target=&#34;_blank&#34;&gt;media coverage&lt;/a&gt;&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;I worked on machine translation.&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Recent Talks</title>
      <link>//localhost:1313/pages/talks/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/talks/</guid>
      <description>&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1KFNUQvK3NYClYkFNnU775F5BC3Irglw7Pe81Xf_-nJY/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Interpreting Deep NLP Models&lt;/a&gt;, University of Edinburgh, UK (April 2020)&lt;/li&gt;
&lt;li&gt;Summarizing Research on Interpreting Machine Translation Models, University of Sheffield, UK (Mar 2020)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1N2mwnoIhzx7RLPQDmkX2W_J6GDPElHHHYYI7U3mXxBo/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Efficient Transfer Learning of Pretrained Model&lt;/a&gt;, 7th International Conference on Language and Technology, Pakistan (February 2020)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1UL-p4Cj2Sr2DaFguFlNd17MmkDsRHxT2lObt_l8tlFA/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Analyzing Individual Neurons in Deep NLP Models&lt;/a&gt; at Google, Facebook, Amazon, Salesforce and Bosch, US (April 2019)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/12ETd8Ox_xseuD9es2h45DguRJDiJ2qByWkG67RM2rMs/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Hidden Linguistics in Deep NLP Models&lt;/a&gt;, Symposium on Natural Language Processing, University of Moratuwa, Sri Lanka (March 2019)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Teaching</title>
      <link>//localhost:1313/pages/teaching/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/teaching/</guid>
      <description>

&lt;hr /&gt;

&lt;div style=&#34;display: flex;flex-direction: row;align-items: center;&#34;&gt;
  &lt;img src=&#34;//localhost:1313/files/testimonial-dl4mt-1.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 45%;height: 100%;&#34;&gt; 
    &lt;div style=&#34;width:10%&#34;&gt;&lt;/div&gt;
  &lt;img src=&#34;//localhost:1313/files/testimonial-dl4mt-2.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 45%;height: 100%;&#34;&gt; 
&lt;/div&gt;

&lt;h3 id=&#34;deep-learning-for-natural-language-processing&#34;&gt;Deep Learning for Natural Language Processing&lt;/h3&gt;

&lt;p&gt;A 15 hours crash course covering the basics and advancements in deep learning. The lecture series is conducted at the department of Computer Science and Applied Cognitive Science of the &lt;a href=&#34;https://www.uni-due.de/en/&#34; target=&#34;_blank&#34;&gt;University of Duisburg-Essen&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;introduction-to-deep-learning-for-natural-language-processing&#34;&gt;Introduction to Deep Learning for Natural Language Processing&lt;/h3&gt;

&lt;p&gt;A one week introductory course delivered at the DAAD organized Spring School, University of Moratuwa, Sri Lanka in March 2019.&lt;/p&gt;

&lt;h3 id=&#34;from-theory-to-practice-deep-learning-for-natural-language-processing-hahahugoshortcode-s0-hbhb&#34;&gt;&lt;a href=&#34;//localhost:1313/pages/dl4nlp/&#34; target=&#34;_blank&#34;&gt;From Theory to Practice: Deep Learning for Natural Language Processing&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;A 15 hours crash course on deep learning for NLP with practical exercises in Keras. The lecture series is conducted at the department of Computer Science and Applied Cognitive Science of the &lt;a href=&#34;https://www.uni-due.de/en/&#34; target=&#34;_blank&#34;&gt;University of Duisburg-Essen&lt;/a&gt;. Here is the course material including slides, python notebooks, etc. &lt;a href=&#34;//localhost:1313/pages/dl4nlp/&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;deep-learning-for-machine-translation-hahahugoshortcode-s2-hbhb&#34;&gt;&lt;a href=&#34;//localhost:1313/pages/dl4mt/&#34; target=&#34;_blank&#34;&gt;Deep Learning for Machine Translation&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;I with &lt;a href=&#34;https://fdalvi.github.io&#34; target=&#34;_blank&#34;&gt;Fahim Dalvi&lt;/a&gt; delivered a 15 hours intensive course on deep learning for machine translation in September 2017 at &lt;a href=&#34;http://cl-fallschool2017.phil.hhu.de/&#34; target=&#34;_blank&#34;&gt;DGfS-CL Fall School&lt;/a&gt; in &lt;a href=&#34;https://www.uni-duesseldorf.de/home/startseite.html&#34; target=&#34;_blank&#34;&gt;Heinrich Heine Universität Düsseldorf&lt;/a&gt;. We covered neural networks, language models, recurrent neural network and how they fit in to become a neural machine translation.
&lt;a href=&#34;//localhost:1313/pages/dl4mt/&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
  </channel>
</rss>
