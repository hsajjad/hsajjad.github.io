<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Pages | Hassan Sajjad</title>
    <link>//localhost:1313/pages/</link>
      <atom:link href="//localhost:1313/pages/index.xml" rel="self" type="application/rss+xml" />
    <description>Pages</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 28 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>//localhost:1313/img/icon-192.png</url>
      <title>Pages</title>
      <link>//localhost:1313/pages/</link>
    </image>
    
    <item>
      <title>News</title>
      <link>//localhost:1313/pages/news/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/news/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Accepted Papers 2025:&lt;/strong&gt; 2 papers (ICML), 1 paper (ICLR), 1 (IJMIR)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted Papers 2024:&lt;/strong&gt; 2 papers (NeurIPS), 2 papers (NAACL), 2 papers (EMNLP)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted Papers 2023:&lt;/strong&gt; 3 conference papers (NeurIPS, ICLR, ACL), 1 Journal (JMLR), 3 demo papers (AAAI, ACL, EACL)&lt;/li&gt;
&lt;li&gt;Neurips 2023: Evaluating Neuron Interpretation Methods of NLP Models&lt;/li&gt;
&lt;li&gt;ICLR 2023: Learning Uncertainty for Unknown Domains with Zero-Target-Assumption&lt;/li&gt;
&lt;li&gt;JMLR 2023: Discovering Salient Neurons in deep NLP models&lt;/li&gt;
&lt;li&gt;ACL Demo 2023: NeuroX 2.0 Library for Neuron Analysis of Deep NLP Models&lt;/li&gt;
&lt;li&gt;ACL findings 2023: Impact of Adversarial Training on Robustness and Generalizability of Language Models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted Papers 2022:&lt;/strong&gt; 3 journal articles (1 TACL, 1 CSL, 1 IJDRR), 5 conference papers (1 ICLR, 1 NAACL, 1 COLING, 2 EMNLP), 1 workshop paper (BlackboxNLP)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;NeuroX Toolkit on pip:&lt;/strong&gt; &lt;a href=&#34;https://pypi.org/project/neurox/&#34; target=&#34;_blank&#34;&gt;https://pypi.org/project/neurox/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tutorial @NAACL 2021:&lt;/strong&gt; &lt;a href=&#34;https://github.com/hsajjad/Interpretability-Tutorial-NAACL2021&#34; target=&#34;_blank&#34;&gt;Fine-grained Interpretation and Causation Analysis in Deep NLP Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted Papers 2021:&lt;/strong&gt; 1 TACL, 1 NAACL, 1 ACL findings, 2 ICWSM, 1 IWCS&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; Exploiting Redundancy in Pre-trained Models for Efficient Transfer Learning. Facebook (Feb. 2021), Machine Learning and Data Analytics Symposium (Mar. 2021), National Research Council, Canada&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; Hidden Linguistics in Deep NLP Models. Heinrich-Heine Universität Düsseldorf, Germany&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @COLING 2020:&lt;/strong&gt; Two long papers, 1) Dialect Arabic machine translation, 2) Location mention recognition&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @EMNLP 2020:&lt;/strong&gt; Two long papers on interpretabiliy are accepted. Details coming soon!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media Coverage:&lt;/strong&gt; One billion tokens translated. &lt;a href=&#34;https://www.gulf-times.com/story/671869/QCRI-s-Shaheen-achieves-milestone-with-over-1bn-words-translated&#34; target=&#34;_blank&#34;&gt;Gulf times&lt;/a&gt;, &lt;a href=&#34;https://www.qatar-tribune.com/news-details/id/196901/qcri-s-shaheen-celebrates-milestone-with-over-1-billion-translated-words&#34; target=&#34;_blank&#34;&gt;Qatar tribune&lt;/a&gt; and several TV/radio channels such as Qatar radio and Qatar TV&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Released:&lt;/strong&gt; First-ever &lt;a href=&#34;http://mt.qcri.org/api&#34; target=&#34;_blank&#34;&gt;Dialectal Arabic to English Machine Translation System&lt;/a&gt; covering a large variety of Arabic dialects&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tech Transfer:&lt;/strong&gt; &lt;a href=&#34;http://mt.qcri.org/api&#34; target=&#34;_blank&#34;&gt;Machine Translation&lt;/a&gt; Technology to &lt;a href=&#34;http://www.kanerai.com&#34; target=&#34;_blank&#34;&gt;KanariAI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @ACL 2020:&lt;/strong&gt; Similarity Analysis of Contextual Word Representation Models&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; Interpreting Deep NLP Models, University of Edinburgh, UK (April 2020)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; University of Sheffield, UK (Mar 2020)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Invited talk:&lt;/strong&gt; Efficient Transfer Learning of Pretrained Model. 7th International Conference on Language and Technology, Pakistan (February 2020)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; Analyzing Individual Neurons in Deep NLP Models at Google, Facebook, Amazon, Salesforce and Bosch, US (April 2019)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Keynote:&lt;/strong&gt; Hidden Linguistic in Deep NLP Models, Symposium on Natural Language Processing, University of Moratuwa, Sri Lanka (March 2019)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep learning for NLP:&lt;/strong&gt; Fun teaching at International Spring School in Advanced Language Engineering, University of Moratuwa (March 2019)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; Machine Translation in Real World, King&amp;rsquo;s College London, UK (March 2019)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Talk:&lt;/strong&gt; Analyzing Individual Neurons in Deep NLP Models, University of Melbourne, Australia (February 2019)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @NAACL 2019:&lt;/strong&gt; &amp;ldquo;One Size Does Not Fit All: Comparing NMT Representations of Different Granularities&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @NAACL 2019:&lt;/strong&gt; &amp;ldquo;Highly Effective Arabic Diacritization using Sequence to Sequence Modeling&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media coverage:&lt;/strong&gt; Our work on NeuroX - Analyzing and Controlling Individual Neurons is featured at &lt;a href=&#34;https://news.mit.edu/2019/neural-networks-nlp-microscope-0201&#34; target=&#34;_blank&#34;&gt;MIT news&lt;/a&gt; and several AI blogs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @ICLR 2019:&lt;/strong&gt; &amp;ldquo;Identifying and Controlling Important Neurons in Neural Machine Translation&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @AAAI 2019:&lt;/strong&gt; &amp;ldquo;What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @AAAI 2019:&lt;/strong&gt; &amp;ldquo;NeuroX: A toolkit for Analyzing Individual Neurons in Neural Network&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Best Innovation Award&lt;/strong&gt; @&lt;a href=&#34;https://www.qf-arc.org/en-us/ARC-18&#34; target=&#34;_blank&#34;&gt;ARC&amp;rsquo;18&lt;/a&gt; for our &lt;a href=&#34;http://st.qcri.org&#34; target=&#34;_blank&#34;&gt;Speech Translation System&lt;/a&gt;, see &lt;a href=&#34;http://www.gulf-times.com/story/585737/Sheikha-Moza-honours-QF-ARC-18-award-winners&#34; target=&#34;_blank&#34;&gt;media coverage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep learning for NLP:&lt;/strong&gt; I will be delivering an intensive course on deep learning for NLP at the University of Duisburg-Essen, Germany in the second week of April 2018&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @NAACL 2018:&lt;/strong&gt; Incremental decoding and training methods for simultaneous translation in neural machine translation&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Media coverage:&lt;/strong&gt; Our work on understanding Neural Machine Translation has made it to &lt;a href=&#34;http://news.mit.edu/2017/reading-neural-network-mind-1211&#34; target=&#34;_blank&#34;&gt;MIT news&lt;/a&gt; and been picked by several channels like &lt;a href=&#34;https://scienceblog.com/498095/reading-neural-networks-mind/&#34; target=&#34;_blank&#34;&gt;ScienceBlog&lt;/a&gt;, &lt;a href=&#34;https://www.sciencedaily.com/releases/2017/12/171211120131.htm&#34; target=&#34;_blank&#34;&gt;ScienceDaily&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted @IWSLT 2017:&lt;/strong&gt; Neural Machine Translation Training in a Multi-domain Scenario&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Deep learning for MT:&lt;/strong&gt; I had great fun in teaching deep learning for machine translation at the &lt;a href=&#34;http://cl-fallschool2017.phil.hhu.de/&#34; target=&#34;_blank&#34;&gt;DGfS-CL Fall School&lt;/a&gt;. Course material is available &lt;a href=&#34;http://cl-fallschool2017.phil.hhu.de/index.php/courses/course-2/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accepted two papers @ACL 2017&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Read my &lt;a href=&#34;https://www.linkedin.com/pulse/what-do-neural-machine-translation-models-learn-hassan-sajjad/&#34; target=&#34;_blank&#34;&gt;post&lt;/a&gt; about What does Neural Machine Translation Learn about Morphology&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning for Machine Translation</title>
      <link>//localhost:1313/pages/dl4mt/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/dl4mt/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Lecturers: &lt;a href=&#34;https://hsajjad.github.io/&#34; target=&#34;_blank&#34;&gt;Hassan Sajjad&lt;/a&gt; and &lt;a href=&#34;https://fdalvi.github.io/&#34; target=&#34;_blank&#34;&gt;Fahim Dalvi&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;In this lecture series, we first cover the basics of statistical machine translation to establish the intuition behind machine translation. We then cover the basics of neural network models - word embedding and neural language model. Finally, we learn an end-to-end translation system based completely on deep neural networks. In the last part of the lecture series, we learn to peek into these neural systems and analyze what they learn about the intricacies of a language like morphology and syntax, without ever explicitly seeing these details in the training data.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Background reading&lt;/strong&gt;
* &lt;a href=&#34;https://cs231n.github.io/python-numpy-tutorial/&#34; target=&#34;_blank&#34;&gt;Python Numpy Tutorial&lt;/a&gt;
* &lt;a href=&#34;https://cs231n.github.io/ipython-tutorial/&#34; target=&#34;_blank&#34;&gt;IPython Tutorial&lt;/a&gt;
* &lt;a href=&#34;http://www.cedar.buffalo.edu/~srihari/CSE574/Chap1/LinearAlgebra.pdf&#34; target=&#34;_blank&#34;&gt;Linear Aljebra for Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lecture 0&lt;/strong&gt; - Introduction &amp;amp; Roadmap &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzYkYxYzJaZUY0anc&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 1&lt;/strong&gt; - Language &amp;amp; Translation &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzZkhrcG9mSWo0cEE&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 2&lt;/strong&gt; - Language Modeling &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzSjhoanNaQTdlNDQ&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzTnc5d1VWa3ItbUU&#34; target=&#34;_blank&#34;&gt;Python tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzMDRKbTNWWk96V3M&#34; target=&#34;_blank&#34;&gt;Python tutorial as a PDF&lt;/a&gt; [non-editable]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 3&lt;/strong&gt; - Machine Learning &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzWDhUbDV6T3hZTDg&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzU2RjOFFQUW5IQk0&#34; target=&#34;_blank&#34;&gt;Decision Boundary Exercise&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzYUxKMGY2ZkluQ3c&#34; target=&#34;_blank&#34;&gt;Optimization functions demonstration&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 4&lt;/strong&gt; - Machine Learning II &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzSllqVWFXemUyNk0&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzTDVjdWJ2NFNxaTQ&#34; target=&#34;_blank&#34;&gt;Linear Classifier with MSE&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzeVBDOWJ0ckhQQjg&#34; target=&#34;_blank&#34;&gt;Linear Classifier with Softmax and Cross Entropy&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 5&lt;/strong&gt; - Machine Learning and Neural Networks &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzSmJjR2wwZjJjbEU&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzQVZtVmZsNTNqNE0&#34; target=&#34;_blank&#34;&gt;Efficient Linear Classifier with Softmax and Cross Entropy&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzwwwL3SyiJYVlAta1JhSk5pLXc&#34; target=&#34;_blank&#34;&gt;Neural Network&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzLU00MjZJQ2tXa1k&#34; target=&#34;_blank&#34;&gt;Neural Network with Keras toolkit&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 6&lt;/strong&gt; - Neural Network Language Models &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzazUyZjdPVkZYRGc&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzMVE4d1o0YlVtbzA&#34; target=&#34;_blank&#34;&gt;Language Modeling with Keras&lt;/a&gt; [iPython/Jupyter Notebook]&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 7&lt;/strong&gt; - Sequence to Sequence &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzdjIwSWI2Mnk1ZHc&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 8&lt;/strong&gt; - Practical Neural MT &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzNjFSOUYwZ0txUzA&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 9&lt;/strong&gt; - Analysis of Neural MT &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzME1GWVlnWUdCREU&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Lecture 10&lt;/strong&gt; - Recent Advancements &lt;a href=&#34;https://drive.google.com/open?id=0BzdJvZytWnuzS0xPWFlOT0ZOeG8&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning for Natural Language Processing</title>
      <link>//localhost:1313/pages/dl4nlp/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/dl4nlp/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Lecturers: &lt;a href=&#34;https://fdalvi.github.io/&#34; target=&#34;_blank&#34;&gt;Fahim Dalvi&lt;/a&gt; and &lt;a href=&#34;https://hsajjad.github.io/&#34; target=&#34;_blank&#34;&gt;Hassan Sajjad&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;In this lecture series, we cover the basics of machine learning, neural networks and deep neural networks. We look at several deep neural network architectures from the perspective of applying them to various classification tasks, such as sequence prediction and generation. Every lecture is accompanied with practice problems implemented in Keras, a popular Python framework for deep learning.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Background reading&lt;/strong&gt;
* &lt;a href=&#34;https://cs231n.github.io/python-numpy-tutorial/&#34; target=&#34;_blank&#34;&gt;Python Numpy Tutorial&lt;/a&gt;
* &lt;a href=&#34;https://cs231n.github.io/ipython-tutorial/&#34; target=&#34;_blank&#34;&gt;IPython Tutorial&lt;/a&gt;
* &lt;a href=&#34;http://www.cedar.buffalo.edu/~srihari/CSE574/Chap1/LinearAlgebra.pdf&#34; target=&#34;_blank&#34;&gt;Linear Aljebra for Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lecture 0&lt;/strong&gt; - Introduction &amp;amp; Roadmap &lt;a href=&#34;https://drive.google.com/open?id=1SUaoBDjAoFG2f_SAX2FNqM4Wcs3JeXbs&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 1&lt;/strong&gt; - Introduction to Machine Learning &lt;a href=&#34;https://drive.google.com/open?id=1U0289zrCDrABLpTZOUba_IcDoApq1tZT&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Practical session &lt;a href=&#34;https://drive.google.com/open?id=1nZTTXiHOlXo5HO4wRy88I5Sk0-_egGwI&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Learning Rate and Optimization Demo &lt;a href=&#34;https://drive.google.com/open?id=1VDBHBh_hGRiiZfIL61b7-RRm6r05Hkg0&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Introduction to Jupyter Notebooks &lt;a href=&#34;https://drive.google.com/open?id=15re_PMcGbg-I-UULld_FJZIIT6OC53A6&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Introduction to Python and Numpy &lt;a href=&#34;https://drive.google.com/open?id=178OeAeD_n-T_MChwiEbkbvZfquko3EZh&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linear Classification by Regression &lt;a href=&#34;https://drive.google.com/open?id=1PbIKa_u4Y19heQOa0dIenZWODNi354W-&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Binary Linear Classification &lt;a href=&#34;https://drive.google.com/open?id=1GIBoQxs1zlAauI8fY5vX2LDDTtkUXx8C&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linear Classification on Spiral Data &lt;a href=&#34;https://drive.google.com/open?id=1ZkmhRrI6eISdJppCVTKJ5cbhVYDg8uU5&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Supplementary Material &lt;a href=&#34;https://drive.google.com/open?id=10RjjHIY9W38YwzwiJ_6CPI19qpP1_4L_&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 2&lt;/strong&gt; - Neural Networks &lt;a href=&#34;https://drive.google.com/open?id=19m3Cf6_AqjmAZZi-n1gMfwYqUdIaFw7G&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Practical session &lt;a href=&#34;https://drive.google.com/open?id=1sYQLeSDxl_i7s5mzoeY5b6kPuxLP36d1&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Networks on Spiral Data &lt;a href=&#34;https://drive.google.com/open?id=1vAGPhE1w7Vc3u0OBBSpdCdwzEqa6Vl0H&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Data &lt;a href=&#34;https://drive.google.com/open?id=1V0ioyI79Qf-Bc4FTs6MFylTGWDMzwMbZ&#34; target=&#34;_blank&#34;&gt;.zip&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sentiment Classification using Neural Networks &lt;a href=&#34;https://drive.google.com/open?id=11CticibncR7YqF40CwC8I2qUqmMfuRwn&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Neural Network Language Model &lt;a href=&#34;https://drive.google.com/open?id=1uWQ8NWtKqESY5WC7VRrz9fnRJFykmos3&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 3&lt;/strong&gt; - Recurrent Neural Networks &lt;a href=&#34;https://drive.google.com/open?id=1jeQCTTxZkJ0E1R9kjGv1Upx4y2g1iMz5&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;Practical Session &lt;a href=&#34;https://drive.google.com/open?id=1_AURYVRlVsEpIrK1yzkho_rO1E4mQRfD&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Recurrent Neural Networks &lt;a href=&#34;https://drive.google.com/open?id=1_AURYVRlVsEpIrK1yzkho_rO1E4mQRfD&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hybrid Model &lt;a href=&#34;https://drive.google.com/open?id=1fe5HKrE2KMrLUM68ObrjM1gbB_dSRZE8&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 4&lt;/strong&gt; - Sequence to Sequence Models and Practical Considerations

&lt;ul&gt;
&lt;li&gt;Sequence to Sequence Models &lt;a href=&#34;https://drive.google.com/open?id=19PtpBXrIoj7v2Ux-Tw8zmnd5CNTK59OH&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Practical Considerations &lt;a href=&#34;https://drive.google.com/open?id=1V3XD5_aLpbTOP90mgqsL-EtxZL2PYHME&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;
&lt;!---  * Practical Session - Per Timestep Prediction  [English POS tagged data](#)  (https://drive.google.com/open?id=1iaN-h5oepAfeWas6bMH8t4RzbBkOccR9)  ---&gt;&lt;/li&gt;
&lt;li&gt;Per Timestep Prediction &lt;a href=&#34;https://drive.google.com/open?id=13L75QInaEmpF12YptSvGqvKwLGgytj7k&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pretrained Embeddings &lt;a href=&#34;https://drive.google.com/open?id=1fiYX7cXKSZhcpNuuO_vkvOO_QDyIEEWT&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Imbalanced Classes &lt;a href=&#34;https://drive.google.com/open?id=1ND2Gqjo5mDuG0H9O58yLuMsNsKhWC2g1&#34; target=&#34;_blank&#34;&gt;JupyterNotebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lecture 5&lt;/strong&gt; - Advanced Topics, CNN, Multitask, GAN, RL, etc. &lt;a href=&#34;https://drive.google.com/open?id=1vimK8YQXJLP1z2dxZUHph-pW40hJ8FEg&#34; target=&#34;_blank&#34;&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Interpretation of Deep NLP Models</title>
      <link>//localhost:1313/pages/interpretation/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/interpretation/</guid>
      <description>

&lt;hr /&gt;

&lt;h3 id=&#34;highlights&#34;&gt;Highlights&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Media coverage:&lt;/strong&gt; Our work on NeuroX - Analyzing and Controlling Individual Neurons is featured at &lt;a href=&#34;https://news.mit.edu/2019/neural-networks-nlp-microscope-0201&#34; target=&#34;_blank&#34;&gt;MIT news&lt;/a&gt; and several AI blogs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Media coverage:&lt;/strong&gt; Our work on understanding Neural Machine Translation has made it to &lt;a href=&#34;http://news.mit.edu/2017/reading-neural-network-mind-1211&#34; target=&#34;_blank&#34;&gt;MIT news&lt;/a&gt; and been picked by several channels like &lt;a href=&#34;https://scienceblog.com/498095/reading-neural-networks-mind/&#34; target=&#34;_blank&#34;&gt;ScienceBlog&lt;/a&gt;, &lt;a href=&#34;https://www.sciencedaily.com/releases/2017/12/171211120131.htm&#34; target=&#34;_blank&#34;&gt;ScienceDaily&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;p&gt;I am interested in understanding the learning dynamics of deep neural network models. I have worked on analyzing whole vector representations and individual neurons in the network and answer questions such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How much linguistic knowledge is learned?&lt;/li&gt;
&lt;li&gt;How focused and distributed is the information?&lt;/li&gt;
&lt;li&gt;What is the role of individual neurons?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I showed that the interpretation analysis enables us to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;control bias in our models by manipulating individual neurons&lt;/li&gt;
&lt;li&gt;reduce the model size and speed up inference time by removing irrelevant and redundant neurons&lt;/li&gt;
&lt;li&gt;improve model performance by injecting linguistic information in a multitask setting&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The interpretation work is mainly done in collaboration with CSAIL MIT. The work has been published at prestigious venues, such as ICLR, AAAI, ACL, etc., and it has been covered by several technology blogs a couple of times.&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>Machine Translation</title>
      <link>//localhost:1313/pages/mt/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/mt/</guid>
      <description>

&lt;hr /&gt;

&lt;h4 id=&#34;highlights&#34;&gt;Highlights&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://st.qcri.org/demos/livetranslation/&#34; target=&#34;_blank&#34;&gt;Live Speech Translation&lt;/a&gt; System&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://mt.qcri.org/api/&#34; target=&#34;_blank&#34;&gt;Machine Translation&lt;/a&gt; System&lt;/p&gt;

&lt;p&gt;First industry scale &lt;a href=&#34;https://mt.qcri.org/api/&#34; target=&#34;_blank&#34;&gt;dialectal Arabic to English machine translation system&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Machine Translation licensed to &lt;a href=&#34;http://www.kanerai.com&#34; target=&#34;_blank&#34;&gt;KanariAI&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://st.qcri.org/demos/livetranslation/&#34; target=&#34;_blank&#34;&gt;Live Speech Translation&lt;/a&gt; won the best Innovation award at @&lt;a href=&#34;https://www.qf-arc.org/en-us/ARC-18&#34; target=&#34;_blank&#34;&gt;ARC&amp;rsquo;18&lt;/a&gt;, see &lt;a href=&#34;http://www.gulf-times.com/story/585737/Sheikha-Moza-honours-QF-ARC-18-award-winners&#34; target=&#34;_blank&#34;&gt;media coverage&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;I have worked on both statistical and neural machine translation, involving several language pairs such as English, German, Russian, Arabic, Hebrew, etc. I have been interested in improving the translation of resource-poor languages and morphologically-rich languages. Additionally, I worked on domain adaptation and the handling of unknown words. My research work has been published in top tier venues such as ACL, NAACL, EMNLP, etc.&lt;/p&gt;

&lt;p&gt;In addition to research, I have expertise in building industry-grade and customized machine translation systems. As of July 2020, &lt;a href=&#34;https://mt.qcri.org/api/&#34; target=&#34;_blank&#34;&gt;our system&lt;/a&gt; has translated 950 million tokens. The system has been used by Aljazeera, BBC, and DW, and is deployed as part of the H2020 SUMMA project.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Opportunities</title>
      <link>//localhost:1313/pages/opportunities/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/opportunities/</guid>
      <description>

&lt;p&gt;I have several open PhD positions and research assistant positions. Please read the following before reaching out to me.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;phd-positions&#34;&gt;PhD positions&lt;/h3&gt;

&lt;p&gt;I have several open PhD positions. &lt;strong&gt;Deadline&lt;/strong&gt; to apply is 5th October, 2022. Only shortlisted candidates will be contacted.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Research Area - Deep learning, NLP, Responsible AI, Explainable AI&lt;/li&gt;
&lt;li&gt;Qualification - Bachelor or Masters in CS or related discipline&lt;/li&gt;
&lt;li&gt;Research Experience - at least one of the following

&lt;ul&gt;
&lt;li&gt;One or more research papers involving any of the above mentioned research area or the related areas&lt;/li&gt;
&lt;li&gt;Internship or experience working in a research or technology lab&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The undergraduate candidates with exceptional profiles have an option to directly join the PhD.&lt;/p&gt;

&lt;h3 id=&#34;ra-positions-for-dalhousie-students-only&#34;&gt;RA positions (for Dalhousie students only)&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Research position

&lt;ul&gt;
&lt;li&gt;Research work - Deep learning, NLP and interpreting models&lt;/li&gt;
&lt;li&gt;Duration - four to six months&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Frontend/Backend developer

&lt;ul&gt;
&lt;li&gt;Design a webpage for the research group&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;contact&#34;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Please have a meaningful subject of your email e.g. Inquiring about a PhD position&lt;/li&gt;
&lt;li&gt;Share your updated CV and the latest transcript&lt;/li&gt;
&lt;li&gt;write me at hsajjad with domain dal.ca&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>//localhost:1313/pages/bio/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/bio/</guid>
      <description>&lt;p&gt;My name is Hassan Sajjad. I am an NLP researcher, consultant and coach blended with entrepreneurial interests.&lt;/p&gt;

&lt;p&gt;Currently, I am working as a research scientist in &lt;a href=&#34;http://qcri.com/our-research/arabic-language-technologies&#34; target=&#34;_blank&#34;&gt;Arabic Language Technologies group&lt;/a&gt; at &lt;a href=&#34;http://qcri.com/&#34; target=&#34;_blank&#34;&gt;Qatar Computing Research Institute&lt;/a&gt;, Qatar. I manage applied machine learning team that mainly work on interpretation of deep NLP models, machine translation and transfer learning.&lt;/p&gt;

&lt;p&gt;Following is a summary of my areas of interest:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Research Interests&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Applied deep learning and machine learning, unsupervised and semi-supervised learning methods, interpretability and manipulation of neural models, multi-task learning, transfer learning&lt;/li&gt;
&lt;li&gt;Natural language processing, statistical and neural machine translation, transliteration, domain adaptation, NLP for resource poor languages, and user-generated content processing and analysis&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Application Interests&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Building large scale practical systems, issues related to deployment of models, problem solving from the perspective of end user, machine translation competitions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Coaching Interests&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Deep learning from scratch, explanation of models by developing intuition from real world examples, making understanding theory easy with animations, practical insight of deep learning models&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Entrepreneurial Interests&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Lean startup, technology transfer, business development, customer validation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Coming back to my bio, &lt;strong&gt;other than research work at QCRI&lt;/strong&gt; I manage applied machine learning group at QCRI, lead deployment and commercialization efforts of QCRI&amp;rsquo;s machine translation technology, managed Arabic machine translation part of the European Project on media monitoring, &lt;a href=&#34;http://summa-project.eu/&#34; target=&#34;_blank&#34;&gt;SUMMA&lt;/a&gt; and the collaboration with &lt;a href=&#34;http://web.mit.edu/&#34; target=&#34;_blank&#34;&gt;MIT&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Before coming to QCRI&lt;/strong&gt;, I graduated from University of Stuttgart in Fall 2012 under the supervision of &lt;a href=&#34;http://www.cis.uni-muenchen.de/schuetze/&#34; target=&#34;_blank&#34;&gt;Prof. Dr. Hinrich Schütze&lt;/a&gt;. There I closely worked with &lt;a href=&#34;http://www.cis.uni-muenchen.de/~fraser/&#34; target=&#34;_blank&#34;&gt;Alex Fraser&lt;/a&gt;, &lt;a href=&#34;http://www.cis.uni-muenchen.de/~schmid/&#34; target=&#34;_blank&#34;&gt;Helmut Schmid&lt;/a&gt; and &lt;a href=&#34;http://alt.qcri.org/~ndurrani/&#34; target=&#34;_blank&#34;&gt;Nadir Durrani&lt;/a&gt;. In my PhD, I worked on part-of-speech tagging, statistical machine translation, and unsupervised models for transliteration mining. I also worked at &lt;a href=&#34;https://www.microsoft.com/en-us/research/&#34; target=&#34;_blank&#34;&gt;Microsoft Research&lt;/a&gt; as an intern where I worked on query expansion for natural language question generation. Before starting PhD, I did &lt;strong&gt;bachelors and masters&lt;/strong&gt; in Computer Science from &lt;a href=&#34;http://www.nu.edu.pk/&#34; target=&#34;_blank&#34;&gt;National University of Computer and Emerging Sciences&lt;/a&gt;, Pakistan. In my master thesis, I worked on Part of Speech tagging of Urdu language under the supervision of &lt;a href=&#34;http://www.cle.org.pk/information/people/drsarmadhussain.html&#34; target=&#34;_blank&#34;&gt;Dr. Sarmad Hussain&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Research</title>
      <link>//localhost:1313/pages/research/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/research/</guid>
      <description>

&lt;p&gt;I worked on several NLP problems in general. However, the two dominating areas where I worked the most are; machine translation and interpretation of deep NLP models. Here, I have first summarized my &lt;a href=&#34;#research-interests&#34;&gt;research interests&lt;/a&gt;. Later, I provide a summary of my work on &lt;a href=&#34;#machine-translation&#34;&gt;Machine Translation&lt;/a&gt; and &lt;a href=&#34;#interpretation&#34;&gt;Interpretation&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;research-interests&#34;&gt;Research Interests&lt;/h3&gt;

&lt;p&gt;In the following, I provide a non-exhaustive summary of my areas of interest:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Research Interests&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Applied deep learning and machine learning, unsupervised and semi-supervised learning methods, interpretability and manipulation of neural models, generalization, multi-task learning, transfer learning, representation learning, efficient modeling&lt;/li&gt;
&lt;li&gt;Natural language processing, statistical and neural machine translation, transliteration, domain adaptation, NLP for resource poor languages, and social media content processing and analysis&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Application Interests&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Building large scale practical systems, issues related to deployment of models, problem solving from the perspective of end user, machine translation competitions&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Coaching Interests&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Deep learning from scratch, explanation of models by developing intuition from real world examples, making understanding theory easy with animations, practical insight of deep learning models&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Entrepreneurial Interests&lt;/strong&gt;

&lt;ul&gt;
&lt;li&gt;Lean startup, technology transfer, business development, customer validation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;#top&#34;&gt;Back to top&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;interpretation&#34;&gt;Interpretation&lt;/h3&gt;

&lt;h4 id=&#34;highlights&#34;&gt;Highlights&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Media coverage:&lt;/strong&gt; Our work on NeuroX - Analyzing and Controlling Individual Neurons is featured at &lt;a href=&#34;https://news.mit.edu/2019/neural-networks-nlp-microscope-0201&#34; target=&#34;_blank&#34;&gt;MIT news&lt;/a&gt; and several AI blogs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Media coverage:&lt;/strong&gt; Our work on understanding Neural Machine Translation has made it to &lt;a href=&#34;http://news.mit.edu/2017/reading-neural-network-mind-1211&#34; target=&#34;_blank&#34;&gt;MIT news&lt;/a&gt; and been picked by several channels like &lt;a href=&#34;https://scienceblog.com/498095/reading-neural-networks-mind/&#34; target=&#34;_blank&#34;&gt;ScienceBlog&lt;/a&gt;, &lt;a href=&#34;https://www.sciencedaily.com/releases/2017/12/171211120131.htm&#34; target=&#34;_blank&#34;&gt;ScienceDaily&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I am interested in interpreting and understanding the learning dynamics of deep neural network models. I have worked on analyzing whole vector representations and individual neurons in the network and answer questions such as:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How much linguistic knowledge is learned?&lt;/li&gt;
&lt;li&gt;How focused and distributed is the information?&lt;/li&gt;
&lt;li&gt;What is the role of individual neurons?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I showed that the interpretation analysis enables us to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;control bias in our models by manipulating individual neurons&lt;/li&gt;
&lt;li&gt;reduce the model size and speed up inference time by removing irrelevant and redundant neurons&lt;/li&gt;
&lt;li&gt;improve model performance by injecting linguistic information in a multitask setting&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The interpretation work is mainly done in collaboration with CSAIL MIT. The work has been published at prestigious venues, such as ICLR, AAAI, ACL, etc., and it has been covered by several technology blogs a couple of times.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;#top&#34;&gt;Back to top&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;machine-translation&#34;&gt;Machine Translation&lt;/h3&gt;

&lt;h4 id=&#34;highlights-1&#34;&gt;Highlights&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://st.qcri.org/demos/livetranslation/&#34; target=&#34;_blank&#34;&gt;Live Speech Translation&lt;/a&gt; System&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://mt.qcri.org/api/&#34; target=&#34;_blank&#34;&gt;Machine Translation&lt;/a&gt; System&lt;/p&gt;

&lt;p&gt;First industry scale &lt;a href=&#34;https://mt.qcri.org/api/&#34; target=&#34;_blank&#34;&gt;dialectal Arabic to English machine translation system&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Machine Translation licensed to &lt;a href=&#34;http://www.kanerai.com&#34; target=&#34;_blank&#34;&gt;KanariAI&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://st.qcri.org/demos/livetranslation/&#34; target=&#34;_blank&#34;&gt;Live Speech Translation&lt;/a&gt; won the best Innovation award at @&lt;a href=&#34;https://www.qf-arc.org/en-us/ARC-18&#34; target=&#34;_blank&#34;&gt;ARC&amp;rsquo;18&lt;/a&gt;, see &lt;a href=&#34;http://www.gulf-times.com/story/585737/Sheikha-Moza-honours-QF-ARC-18-award-winners&#34; target=&#34;_blank&#34;&gt;media coverage&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I have worked on both statistical and neural machine translation, involving several language pairs such as English, German, Russian, Arabic, Hebrew, etc. I have been interested in improving the translation of resource-poor languages and morphologically-rich languages. Additionally, I worked on domain adaptation and the handling of unknown words. My research work has been published in top tier venues such as ACL, NAACL, EMNLP, etc.&lt;/p&gt;

&lt;p&gt;In addition to research, I have expertise in building industry-grade and customized machine translation systems. As of July 2020, &lt;a href=&#34;https://mt.qcri.org/api/&#34; target=&#34;_blank&#34;&gt;our system&lt;/a&gt; has translated 950 million tokens. The system has been used by Aljazeera, BBC, and DW, and is deployed as part of the H2020 SUMMA project.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;#top&#34;&gt;Back to top&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Talks</title>
      <link>//localhost:1313/pages/talks/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/talks/</guid>
      <description>&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;Latent Space Exploration for Safe and Trustworthy AI Models. AI@Thomson Reuters (Apr. 2024)&lt;/li&gt;
&lt;li&gt;Neuron Interpretation of Deep NLP Models. ITU, Lahore, Pakistan (Dec. 2023)&lt;/li&gt;
&lt;li&gt;Latent Concept based Explanation of Deep Learning Models. MBZUAI, Abu Dhabi (Nov. 2023)&lt;/li&gt;
&lt;li&gt;Latent Concepts in Transformer Models of NLP.  UKP, TU Darmstadt, Germany (Jun. 2023)&lt;/li&gt;
&lt;li&gt;Knowledge Manifolds in Transformer Models of NLP, National Research Council (NRC), Canada (Apr. 2023)&lt;/li&gt;
&lt;li&gt;Knowledge Manifolds in Transformer Models of NLP, University of Ottawa, Canada (Apr. 2023)&lt;/li&gt;
&lt;li&gt;Analyzing Latent Concepts in Deep Neural Network Models of NLP. STCI Microsoft, India (Jun. 2022)&lt;/li&gt;
&lt;li&gt;Analyzing Latent Concepts in Deep Neural Network Models of NLP.  Data Science Institute, National University of Ireland Galway,  (Jun. 2022)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1yWQZ8qUL8EaDeIRqegNQefz_UH4Np-O9WbiNkbyuqLs/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Exploiting Redundancy in Pre-trained Models for Efficient Transfer Learning&lt;/a&gt;, Machine Learning and Data Analytics Symposium, Qatar (Mar. 2021)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1yWQZ8qUL8EaDeIRqegNQefz_UH4Np-O9WbiNkbyuqLs/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Exploiting Redundancy in Pre-trained Models for Efficient Transfer Learning&lt;/a&gt;, Facebook, US (Feb 2021)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1yWQZ8qUL8EaDeIRqegNQefz_UH4Np-O9WbiNkbyuqLs/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Exploiting Redundancy in Pre-trained Models for Efficient Transfer Learning&lt;/a&gt;, National Research Council, Canada (Nov 2020)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1KFNUQvK3NYClYkFNnU775F5BC3Irglw7Pe81Xf_-nJY/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Interpreting Deep NLP Models&lt;/a&gt;, University of Edinburgh, UK (April 2020)&lt;/li&gt;
&lt;li&gt;Summarizing Research on Interpreting Machine Translation Models, University of Sheffield, UK (Mar 2020)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1N2mwnoIhzx7RLPQDmkX2W_J6GDPElHHHYYI7U3mXxBo/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Efficient Transfer Learning of Pretrained Model&lt;/a&gt;, 7th International Conference on Language and Technology, Pakistan (February 2020)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1UL-p4Cj2Sr2DaFguFlNd17MmkDsRHxT2lObt_l8tlFA/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Analyzing Individual Neurons in Deep NLP Models&lt;/a&gt; at Google, Facebook, Amazon, Salesforce and Bosch, US (April 2019)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/12ETd8Ox_xseuD9es2h45DguRJDiJ2qByWkG67RM2rMs/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Hidden Linguistics in Deep NLP Models&lt;/a&gt;, Symposium on Natural Language Processing, University of Moratuwa, Sri Lanka (March 2019)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1X3zEdgOo7TlwxkssI4o1TmUO2txqdFJ5obycIM_D_qE/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Machine Translation in the Real World&lt;/a&gt;, Kings College London, UK (March 2019)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1Ia2fdSZsODKJqUvCZF83hAm8POgo9hbX2xoSPmlmZ3k/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Analyzing Individual Neurons in Deep NLP Models&lt;/a&gt;, University of Melbourne, Melbourne, Australia (Feb. 2019)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1Ia2fdSZsODKJqUvCZF83hAm8POgo9hbX2xoSPmlmZ3k/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Analyzing Individual Neurons in Deep NLP Models&lt;/a&gt;, Thomson Reuters, Toronto, Canada (Feb. 2019)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1310_sysMKfowALnAT-8yuK_ADy6LM61Xzp_L15WiXPM/edit?usp=sharing&#34; target=&#34;_blank&#34;&gt;Research Findings&lt;/a&gt;, NRC, Ottawa, Canada (Feb. 2018)&lt;/li&gt;
&lt;li&gt;What do Neural Machine Translation Models Learn about Morphology, Macquarie University, Sydney, Australia (Apr. 2017)&lt;/li&gt;
&lt;li&gt;What do Neural Machine Translation Models Learn about Morphology, University of Melbourne, Melbourne, Australia (Apr. 2017)&lt;/li&gt;
&lt;li&gt;From Phrase-based to Neural Machine Translation. Workshop on Semitic Machine Translation, AMTA, Austin, US (Nov. 2016) (Keynote)&lt;/li&gt;
&lt;li&gt;Deep Learning – Neural Machine Translation. Sixth Conference on Language and Technology (CLT16), Lahore, Pakistan (Nov. 2016) (Keynote)&lt;/li&gt;
&lt;li&gt;Content Model Applications for Promoting Local Language Content. Workshop on Facilitating Local Language Content Access and Generation using Human Language Technologies, UET, Lahore, Pakistan (Aug. 2015) (Keynote)&lt;/li&gt;
&lt;li&gt;Statistical Machine Translation for Community Service: Translating Educational Content. Fifth Conference on Language and Technology (CLT14), Karachi, Pakistan (Nov. 2014) (Keynote)&lt;/li&gt;
&lt;li&gt;Separating Transliterations from Translations in Transliteration Mining Context. FBK, Trento, Italy (Oct. 2012)&lt;/li&gt;
&lt;li&gt;Unsupervised Transliteration Mining. School of Science and Engineering, Lahore University of Management and Sciences, Pakistan (Apr. 2012)&lt;/li&gt;
&lt;li&gt;Unsupervised Transliteration Mining, Punjab University College of Information Technology, Pak- istan (Apr. 2012)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teaching</title>
      <link>//localhost:1313/pages/teaching/</link>
      <pubDate>Sun, 28 Jun 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/pages/teaching/</guid>
      <description>

&lt;hr /&gt;

&lt;div style=&#34;display: flex;flex-direction: row;align-items: center;&#34;&gt;
  &lt;img src=&#34;//localhost:1313/files/testimonial-dl4mt-1.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 45%;height: 100%;&#34;&gt; 
    &lt;div style=&#34;width:10%&#34;&gt;&lt;/div&gt;
  &lt;img src=&#34;//localhost:1313/files/testimonial-dl4mt-2.png&#34; alt=&#34;Drawing&#34; style=&#34;width: 45%;height: 100%;&#34;&gt; 
&lt;/div&gt;

&lt;h3 id=&#34;introduction-to-experimental-robotics&#34;&gt;Introduction to Experimental Robotics&lt;/h3&gt;

&lt;p&gt;Fall 2022, Winter 2023, Dalhousie University&lt;/p&gt;

&lt;h3 id=&#34;deep-learning-for-natural-language-processing&#34;&gt;Deep Learning for Natural Language Processing&lt;/h3&gt;

&lt;p&gt;Winter 2023, Dalhousie University&lt;/p&gt;

&lt;h3 id=&#34;deep-learning-for-natural-language-processing-1&#34;&gt;Deep Learning for Natural Language Processing&lt;/h3&gt;

&lt;p&gt;A 15 hours crash course covering the basics and advancements in deep learning. The lecture series is conducted at the department of Computer Science and Applied Cognitive Science of the &lt;a href=&#34;https://www.uni-due.de/en/&#34; target=&#34;_blank&#34;&gt;University of Duisburg-Essen&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;introduction-to-deep-learning-for-natural-language-processing&#34;&gt;Introduction to Deep Learning for Natural Language Processing&lt;/h3&gt;

&lt;p&gt;A one week introductory course delivered at the DAAD organized Spring School, University of Moratuwa, Sri Lanka in March 2019.&lt;/p&gt;

&lt;h3 id=&#34;from-theory-to-practice-deep-learning-for-natural-language-processing-hahahugoshortcode-s0-hbhb&#34;&gt;&lt;a href=&#34;//localhost:1313/pages/dl4nlp/&#34; target=&#34;_blank&#34;&gt;From Theory to Practice: Deep Learning for Natural Language Processing&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;A 15 hours crash course on deep learning for NLP with practical exercises in Keras. The lecture series is conducted at the department of Computer Science and Applied Cognitive Science of the &lt;a href=&#34;https://www.uni-due.de/en/&#34; target=&#34;_blank&#34;&gt;University of Duisburg-Essen&lt;/a&gt;. Here is the course material including slides, python notebooks, etc. &lt;a href=&#34;../dl4nlp&#34;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;deep-learning-for-machine-translation-hahahugoshortcode-s1-hbhb&#34;&gt;&lt;a href=&#34;//localhost:1313/pages/dl4mt/&#34; target=&#34;_blank&#34;&gt;Deep Learning for Machine Translation&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;I with &lt;a href=&#34;https://fdalvi.github.io&#34; target=&#34;_blank&#34;&gt;Fahim Dalvi&lt;/a&gt; delivered a 15 hours intensive course on deep learning for machine translation in September 2017 at &lt;a href=&#34;http://cl-fallschool2017.phil.hhu.de/&#34; target=&#34;_blank&#34;&gt;DGfS-CL Fall School&lt;/a&gt; in &lt;a href=&#34;https://www.uni-duesseldorf.de/home/startseite.html&#34; target=&#34;_blank&#34;&gt;Heinrich Heine Universität Düsseldorf&lt;/a&gt;. We covered neural networks, language models, recurrent neural network and how they fit in to become a neural machine translation. &lt;a href=&#34;../dl4mt&#34;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
    </item>
    
  </channel>
</rss>
